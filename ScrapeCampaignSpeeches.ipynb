{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis of Presidential speech and election data**\n",
    "\n",
    "This notebook scrapes [The American Presidency Project](http://www.presidency.ucsb.edu) and downloads the campagin speeches of all 2016 presidential candidates.  It then builds a markov chain out each president's data capable of generating sentences in the style of their campaign speeches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from lxml import html\n",
    "from bs4 import BeautifulSoup\n",
    "import markovify\n",
    "import os.path\n",
    "from datetime import datetime, timedelta\n",
    "import calendar\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def getCandidateSpeechLinks(url):\n",
    "    allCandidatePage = requests.get(url)\n",
    "    allCandidatePageSoup = BeautifulSoup(allCandidatePage.text, 'lxml')\n",
    "    links={}\n",
    "    table = allCandidatePageSoup.find('table', width=680)\n",
    "    for area in table.findAll('td', class_='doctext'):\n",
    "        for a in area.findAll('a'):\n",
    "            if ('campaign' in a.text.lower()):\n",
    "                links[area.find('span', class_='roman').text] = a['href']\n",
    "    return links\n",
    "\n",
    "def scrapeCampaignSpeechesToFile(url, path):\n",
    "    allSpeechPages = requests.get(url)\n",
    "    allSpeechSoup=BeautifulSoup(allSpeechPages.text, 'lxml')\n",
    "    root = 'http://www.presidency.ucsb.edu/'\n",
    "    table = allSpeechSoup.find('table', width=700)\n",
    "    links = []\n",
    "    for link in table.findAll('a'):\n",
    "        if('interview' not in link.text.lower()):\n",
    "            links.append(root+(link['href'])[3:])\n",
    "\n",
    "    speechPages = [requests.get(link , 'lxml')for link in links]\n",
    "    speechesSoup = [BeautifulSoup(speechPage.text, 'lxml') for speechPage in speechPages]\n",
    "\n",
    "    with open(path, \"w+\", encoding='utf-8') as outFile:\n",
    "        outFile.seek(0)\n",
    "        for i,speech in enumerate(speechesSoup):            \n",
    "            text = speechesSoup[i].find('span', class_='displaytext').text.replace('.','. ')\n",
    "            text = re.sub('\\[[a-zA-Z]*\\]', ' ', text)\n",
    "            text = re.sub('[A-Z]+ [A-Z]+:', ' ', text)\n",
    "            text = re.sub('\\w+:', ' ', text)\n",
    "            text = re.sub(r'[^\\x00-\\x7F]+',' ', text)\n",
    "            \n",
    "            outFile.write(text +'\\n')\n",
    "\n",
    "\n",
    "def campaignLinkToFiles(url, year):\n",
    "    \n",
    "    dataFolder = './Campaign Speeches/'+ str(year) +'/'\n",
    "    \n",
    "    if not os.path.exists(dataFolder):\n",
    "        os.makedirs(dataFolder)\n",
    "    \n",
    "    #Create the dictionary of each candidate's name and link to their campaign speech page    \n",
    "    campaignSpeechLinkDict = getCandidateSpeechLinks(url)\n",
    "    \n",
    "    root = 'http://www.presidency.ucsb.edu/'\n",
    "    new=0\n",
    "    existing=0\n",
    "    newpaths=[]\n",
    "    #Loops through the campagin speech links, puts each candidate's campagin speeches into individual files\n",
    "    for name, url in campaignSpeechLinkDict.items():\n",
    "        path = dataFolder + name.replace(' ', '-') + '.txt'\n",
    "        if not os.path.isfile(path):\n",
    "            new+=1\n",
    "            newpaths.append(path)\n",
    "            scrapeCampaignSpeechesToFile(root + url, path)\n",
    "        else:\n",
    "            existing+=1\n",
    "    \n",
    "    \n",
    "    \n",
    "    #print some statistics\n",
    "    print(str(existing), ' files already existed, ignoring.')\n",
    "    print(str(new), ' files created successfully:')\n",
    "    for p in newpaths:\n",
    "        print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  files already existed, ignoring.\n",
      "21  files created successfully:\n",
      "./Campaign Speeches/2016/Rick-Perry.txt\n",
      "./Campaign Speeches/2016/Bobby-Jindal.txt\n",
      "./Campaign Speeches/2016/Hillary-Clinton-.txt\n",
      "./Campaign Speeches/2016/Jeb-Bush.txt\n",
      "./Campaign Speeches/2016/George-Pataki.txt\n",
      "./Campaign Speeches/2016/Marco-Rubio.txt\n",
      "./Campaign Speeches/2016/Carly-Fiorina.txt\n",
      "./Campaign Speeches/2016/Rand-Paul.txt\n",
      "./Campaign Speeches/2016/Scott-Walker.txt\n",
      "./Campaign Speeches/2016/Chris-Christie.txt\n",
      "./Campaign Speeches/2016/John-Kasich.txt\n",
      "./Campaign Speeches/2016/Mike-Huckabee.txt\n",
      "./Campaign Speeches/2016/Bernie-Sanders.txt\n",
      "./Campaign Speeches/2016/Rick-Santorum.txt\n",
      "./Campaign Speeches/2016/Lincoln-Chafee.txt\n",
      "./Campaign Speeches/2016/Ted-Cruz.txt\n",
      "./Campaign Speeches/2016/Donald-Trump.txt\n",
      "./Campaign Speeches/2016/Lindsey-Graham.txt\n",
      "./Campaign Speeches/2016/Jim-Webb.txt\n",
      "./Campaign Speeches/2016/Martin-O'Malley.txt\n",
      "./Campaign Speeches/2016/Ben-Carson.txt\n"
     ]
    }
   ],
   "source": [
    "campaignLinkToFiles('http://www.presidency.ucsb.edu/2016_election.php', 2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name,bot in bots2016.items():\n",
    "    print('\\n' + name + ': ')\n",
    "    for i in range(10):\n",
    "        print(bot.make_short_sentence(max_chars=140))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(bots2016['Donald Trump'].make_sentence_with_start(beginning='Crooked'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bots2008 = campaignLinkToBots('http://www.presidency.ucsb.edu/2008_election.php', 2008)\n",
    "\n",
    "for name,bot in bots2008.items():\n",
    "    print('\\n' + name + ': ')\n",
    "    for i in range(10):\n",
    "        print(bot.make_short_sentence(max_chars=140))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
