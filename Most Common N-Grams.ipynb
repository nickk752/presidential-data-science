{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''From https://gist.github.com/benhoyt/dfafeab26d7c02a52ed17b6229f0cb52'''\n",
    "\n",
    "import collections\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "\n",
    "def tokenize(string):\n",
    "    \"\"\"Convert string to lowercase and split into words (ignoring\n",
    "    punctuation), returning list of words.\n",
    "    \"\"\"\n",
    "    return re.findall(r'\\w+', string.lower())\n",
    "\n",
    "def count_ngrams(lines, min_length=2, max_length=4):\n",
    "    \"\"\"Iterate through given lines iterator (file object or list of\n",
    "    lines) and return n-gram frequencies. The return value is a dict\n",
    "    mapping the length of the n-gram to a collections.Counter\n",
    "    object of n-gram tuple and number of times that n-gram occurred.\n",
    "    Returned dict includes n-grams of length min_length to max_length.\n",
    "    \"\"\"\n",
    "    lengths = range(min_length, max_length + 1)\n",
    "    ngrams = {length: collections.Counter() for length in lengths}\n",
    "    queue = collections.deque(maxlen=max_length)\n",
    "\n",
    "    # Helper function to add n-grams at start of current queue to dict\n",
    "    def add_queue():\n",
    "        current = tuple(queue)\n",
    "        for length in lengths:\n",
    "            if len(current) >= length:\n",
    "                ngrams[length][current[:length]] += 1\n",
    "\n",
    "    # Loop through all lines and words and add n-grams to dict\n",
    "    for line in lines:\n",
    "        for word in tokenize(line):\n",
    "            queue.append(word)\n",
    "            if len(queue) >= max_length:\n",
    "                add_queue()\n",
    "\n",
    "    # Make sure we get the n-grams at the tail end of the queue\n",
    "    while len(queue) > min_length:\n",
    "        queue.popleft()\n",
    "        add_queue()\n",
    "\n",
    "    return ngrams\n",
    "\n",
    "def print_most_frequent(ngrams, num=10):\n",
    "    \"\"\"Print num most common n-grams of each length in n-grams dict.\"\"\"\n",
    "    for n in sorted(ngrams):\n",
    "        print('----- {} most common {}-grams -----'.format(num, n))\n",
    "        for gram, count in ngrams[n].most_common(num):\n",
    "            print('{0}: {1}'.format(' '.join(gram), count))\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- 10 most common 2-grams -----\n",
      "of the: 663\n",
      "in the: 590\n",
      "we have: 343\n",
      "it is: 309\n",
      "going to: 307\n",
      "this country: 300\n",
      "that is: 267\n",
      "and the: 264\n",
      "to the: 249\n",
      "that we: 248\n",
      "\n",
      "----- 10 most common 3-grams -----\n",
      "the united states: 164\n",
      "in this country: 135\n",
      "are going to: 91\n",
      "the american people: 87\n",
      "and that is: 86\n",
      "we need to: 83\n",
      "one of the: 83\n",
      "a lot of: 74\n",
      "re going to: 70\n",
      "we are going: 68\n",
      "\n",
      "Took 0.375 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "f = open('./Campaign Speeches/2016/Bernie-Sanders.txt', 'r+', encoding='utf8')\n",
    "ngrams = count_ngrams(f,2,3)\n",
    "print_most_frequent(ngrams)\n",
    "elapsed_time = time.time() - start_time\n",
    "print('Took {:.03f} seconds'.format(elapsed_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
